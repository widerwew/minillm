{
  "model_name": "minillm_pretrain_model",
  "attn_type": "MultiHeadAttentionByPspp",
  "head_nums": 16,
  "eps": 1e-9,
  "dropout": 0.1,
  "use_moe": false,
  "hidden_dim": 512,
  "intermediate_ratio": 2.5,
  "vocab_size": 6400,
  "num_hidden_layers": 16,
  "theta_para": 10000,

  "dataset": "PretrainedDataset",
  "max_length": 8192,
  "learning_rate": 1e-4,
  "batch_size": 16,
  "num_epochs": 5,
  "log_step_interval": 100,
  "seed": 42,
  "data_path": "",
  "tokenizer_path": "",
  "save_path": ""
}