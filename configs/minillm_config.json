{
  "attn_type": "MultiHeadLatentAttention",
  "head_nums": 16,
  "latent_dim": 128,
  "eps": 1e-9,
  "dropout": 0.1,
  "use_moe": true,
  "export_nums": 8,
  "per_token_exports": 3,
  "hidden_dim": 256,
  "embed_dim": 512,
  "vocab_size": 6400,
  "num_layers": 16,
  "max_len": 8192,
  "theta_para": 10000,
  "learning_rate": 1e-4
}